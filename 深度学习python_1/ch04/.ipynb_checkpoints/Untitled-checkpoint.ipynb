{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63620d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.21230986  0.22287786 -1.59666956]\n",
      "2.4124544281148963\n",
      "[[-0.54365871  1.31016551 -1.07640974]\n",
      " [ 0.12653929 -0.6258016  -1.0564708 ]]\n",
      "[[ 0.2146122   0.33163018 -0.54624238]\n",
      " [ 0.3219183   0.49744526 -0.81936357]]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)  # 为了导入父目录中的文件而进行的设定\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n",
    "    \n",
    "    def numerical_gradient_(self):\n",
    "        h=0.0001;\n",
    "        wimd=self.W.flatten();\n",
    "        grad=np.zeros_like(wimd);\n",
    "        \n",
    "        for idx in range(wimd.size):\n",
    "            tmp_val = wimd[idx]\n",
    "            wimd[idx] = float(tmp_val) + h;\n",
    "            fxh1 = f(wimd.reshape(self.W.shape));\n",
    "            wimd[idx] = tmp_val - h;\n",
    "            fxh2 = f(wimd.reshape(self.W.shape));\n",
    "        \n",
    "            grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "            wimd[idx] = tmp_val;\n",
    "        \n",
    "        return wimd.reshape(self.W.shape)\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(net.predict(x))\n",
    "print(f(1))\n",
    "print(net.numerical_gradient_())\n",
    "print(dW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "764fe8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "465bb149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _numerical_gradient_1d(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 还原值\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient_2d(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_1d(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_1d(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 还原值\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5b63b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c=np.max(a);\n",
    "    return np.exp(a-c)/(np.sum(np.exp(a-c)))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ff0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
