{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c56141",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:46:15.342752Z",
     "start_time": "2024-05-06T07:46:14.359860Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import ptb\n",
    "\n",
    "def to_one_hot(t_train, vocab_size):\n",
    "    # 使用NumPy的eye函数创建一个独热编码矩阵\n",
    "    one_hot_matrix = np.eye(vocab_size)[t_train]\n",
    "    return torch.tensor(one_hot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9e2626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:46:16.378936Z",
     "start_time": "2024-05-06T07:46:16.359942Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设定超参数\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNN的隐藏状态向量的元素个数\n",
    "time_size = 35  # RNN的展开大小\n",
    "lr = 20.0\n",
    "max_epoch = 1\n",
    "max_grad = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2037dc6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:46:18.030058Z",
     "start_time": "2024-05-06T07:46:17.975169Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "data_size = len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fc10c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:46:19.908525Z",
     "start_time": "2024-05-06T07:46:19.897904Z"
    }
   },
   "outputs": [],
   "source": [
    "max_iters = data_size // (batch_size * time_size)\n",
    "jump = (data_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "time_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b842f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T06:08:12.949085Z",
     "start_time": "2024-05-04T06:08:12.942540Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fcc832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T02:49:55.510390Z",
     "start_time": "2024-05-04T02:49:55.501719Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cbdf6b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T03:21:00.159223Z",
     "start_time": "2024-05-04T03:21:00.149005Z"
    }
   },
   "source": [
    "# Normal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411942d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T03:47:38.481617Z",
     "start_time": "2024-05-05T03:47:38.455901Z"
    }
   },
   "outputs": [],
   "source": [
    "class lstm_torch(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size, wordvec_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,wordvec_size)\n",
    "        nn.init.xavier_normal_(self.embed.weight)\n",
    "        \n",
    "        self.lstm = nn.LSTM(wordvec_size, hidden_size, batch_first = True)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_hh_l0)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "    \n",
    "    def forward(self, seq, h_0 ,c_0 ):\n",
    "        N , T  = seq.size()\n",
    "        word_embed = self.embed(seq);\n",
    "        out , (h_0 , c_0) = self.lstm(word_embed, (h_0 , c_0))\n",
    "        out = out.reshape(N*T,-1);\n",
    "        out= self.linear(out)\n",
    "        \n",
    "        return out , (h_0 , c_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67686436",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.utils.clip_grad_value_(model.parameters(), clip_value = max_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aecedf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T03:47:46.913554Z",
     "start_time": "2024-05-05T03:47:45.840217Z"
    }
   },
   "outputs": [],
   "source": [
    "model = lstm_torch(vocab_size,wordvec_size,hidden_size)\n",
    "h_0 = torch.zeros((1,batch_size,hidden_size))\n",
    "c_0 = torch.zeros((1,batch_size,hidden_size))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5425e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T03:47:48.507834Z",
     "start_time": "2024-05-05T03:47:48.503787Z"
    }
   },
   "outputs": [],
   "source": [
    "time_idx=0\n",
    "ppl_list = []\n",
    "total_loss = 0\n",
    "loss_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef9559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T02:12:51.790997Z",
     "start_time": "2024-05-05T02:12:26.644899Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "    batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "    batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "    for t in range(time_size):\n",
    "        for i , offset in enumerate(offsets):\n",
    "            batch_x[i,t] = xs[( time_idx + offset) % data_size]\n",
    "            batch_t[i,t] = ts[( time_idx + offset) % data_size]\n",
    "        time_idx +=1\n",
    "        \n",
    "    \n",
    "    batch_x_tensor = torch.tensor(batch_x)\n",
    "    batch_t_tensor = torch.tensor(batch_t)\n",
    "    batch_t_tensor = to_one_hot(batch_t_tensor, vocab_size)\n",
    "    out , (h_0 , c_0) = model(batch_x_tensor, h_0, c_0)\n",
    "    batch_t_tensor = batch_t_tensor.reshape(batch_size*time_size,-1)\n",
    "    \n",
    "    loss = criterion(out, batch_t_tensor) #要保证第二个维度是 vocab_size\n",
    "    h_0 = h_0.detach()\n",
    "    c_0 = c_0.detach()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.nn.utils.clip_grad_value_(model.parameters(), clip_value = max_grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    total_loss += loss\n",
    "    loss_count += 1\n",
    "    \n",
    "    if iter % 20 ==0:\n",
    "        ppl = torch.exp(total_loss / loss_count)\n",
    "        print('epoch {} | iter: {}/1327 | pp: {} | loss:{}'.format(1,iter+1,ppl,loss.item()))\n",
    "        ppl_list.append(float(ppl))\n",
    "        total_loss, loss_count = 0, 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7e4f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:04:50.606990Z",
     "start_time": "2024-05-05T03:47:55.144121Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(4):\n",
    "    for iter in range(max_iters):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i , offset in enumerate(offsets):\n",
    "                batch_x[i,t] = xs[( time_idx + offset) % data_size]\n",
    "                batch_t[i,t] = ts[( time_idx + offset) % data_size]\n",
    "            time_idx +=1\n",
    "        \n",
    "    \n",
    "        batch_x_tensor = torch.tensor(batch_x)\n",
    "        batch_t_tensor = torch.tensor(batch_t)\n",
    "        batch_t_tensor = to_one_hot(batch_t_tensor, vocab_size)\n",
    "        out , (h_0 , c_0) = model(batch_x_tensor, h_0, c_0)\n",
    "        batch_t_tensor = batch_t_tensor.reshape(batch_size*time_size,-1)\n",
    "    \n",
    "        loss = criterion(out, batch_t_tensor) #要保证第二个维度是 vocab_size\n",
    "        \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value = max_grad)\n",
    "    \n",
    "        optimizer.step()\n",
    "        h_0 = h_0.detach()\n",
    "        c_0 = c_0.detach()  #BPTT截断反向传播\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "    \n",
    "        if iter % 20 ==0:\n",
    "            ppl = torch.exp(total_loss / loss_count)\n",
    "            print('epoch {} | iter: {}/1327 | pp: {} | loss:{}'.format(epoch+1,iter+1,ppl,loss.item()))\n",
    "            ppl_list.append(float(ppl))\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9706a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a4d85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:10:52.213862Z",
     "start_time": "2024-05-05T06:10:52.206508Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffed76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:12:28.958073Z",
     "start_time": "2024-05-05T06:12:28.948561Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,2])\n",
    "b = np.array([3,5,7,4])\n",
    "np.intersect1d(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db9d60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:12:30.829261Z",
     "start_time": "2024-05-05T06:12:30.821957Z"
    }
   },
   "outputs": [],
   "source": [
    "np.setdiff1d(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04073ce",
   "metadata": {},
   "source": [
    "# Better LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db36c483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:50:11.193079Z",
     "start_time": "2024-05-06T07:50:11.165849Z"
    }
   },
   "outputs": [],
   "source": [
    "class better_lstm_torch(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size, wordvec_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,wordvec_size)\n",
    "        nn.init.xavier_normal_(self.embed.weight)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(wordvec_size, hidden_size, batch_first = True)\n",
    "        nn.init.xavier_normal_(self.lstm1.weight_hh_l0)\n",
    "        nn.init.xavier_normal_(self.lstm1.weight_ih_l0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first = True)\n",
    "        nn.init.xavier_normal_(self.lstm2.weight_hh_l0)\n",
    "        nn.init.xavier_normal_(self.lstm2.weight_ih_l0)\n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        for para1 , para2 in zip(self.embed.parameters(), self.linear.parameters()):\n",
    "            para2.data = para1.data\n",
    "    \n",
    "    \n",
    "    def forward(self, seq, h_00 ,c_00 , h_01, c_01 ):\n",
    "        N , T  = seq.size()\n",
    "        out = self.embed(seq);\n",
    "        out , (h_00 , c_00) = self.lstm1(out, (h_00 , c_00))\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out , (h_01 , c_01) = self.lstm2(out, (h_01 , c_01))\n",
    "        out = out.reshape(N*T,-1);\n",
    "        out= self.linear(out)\n",
    "        \n",
    "        return out , (h_00 , c_00) , (h_01 , c_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4185c89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:10:18.730692Z",
     "start_time": "2024-05-06T08:10:18.698887Z"
    }
   },
   "outputs": [],
   "source": [
    "model = better_lstm_torch(vocab_size, wordvec_size, hidden_size)\n",
    "h_00,c_00  = (torch.zeros((1,batch_size,hidden_size)),torch.zeros((1,batch_size,hidden_size)))\n",
    "h_01,c_01  = (torch.zeros((1,batch_size,hidden_size)),torch.zeros((1,batch_size,hidden_size)))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "time_idx=0\n",
    "ppl_list = []\n",
    "total_loss = 0\n",
    "loss_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ebc80c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:23:02.583349Z",
     "start_time": "2024-05-06T08:10:22.842860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | iter: 1/1327 | pp: 10102.641803065873 | loss:9.220552233287266\n",
      "epoch 1 | iter: 21/1327 | pp: 1740.611121440743 | loss:7.38252699136734\n",
      "epoch 1 | iter: 41/1327 | pp: 1253.2697669171362 | loss:7.1632459279469085\n",
      "epoch 1 | iter: 61/1327 | pp: 1023.3458556406706 | loss:6.902310350622449\n",
      "epoch 1 | iter: 81/1327 | pp: 874.3320298152369 | loss:6.6685345772334506\n",
      "epoch 1 | iter: 101/1327 | pp: 823.1902769891416 | loss:6.720573621817998\n",
      "epoch 1 | iter: 121/1327 | pp: 879.4086244810204 | loss:6.848347262995584\n",
      "epoch 1 | iter: 141/1327 | pp: 908.572910750185 | loss:6.744910958835057\n",
      "epoch 1 | iter: 161/1327 | pp: 896.6158432647885 | loss:6.595385100500924\n",
      "epoch 1 | iter: 181/1327 | pp: 950.2702753312873 | loss:6.867752318041665\n",
      "epoch 1 | iter: 201/1327 | pp: 823.7810063852896 | loss:6.647412667955671\n",
      "epoch 1 | iter: 221/1327 | pp: 830.8190241254431 | loss:6.748873678616115\n",
      "epoch 1 | iter: 241/1327 | pp: 779.428629366344 | loss:6.619546938964299\n",
      "epoch 1 | iter: 261/1327 | pp: 799.7252962026892 | loss:6.636416968277523\n",
      "epoch 1 | iter: 281/1327 | pp: 841.5769768892521 | loss:6.565864511898586\n",
      "epoch 1 | iter: 301/1327 | pp: 745.8347333827974 | loss:6.509454067434583\n",
      "epoch 1 | iter: 321/1327 | pp: 643.0738544617382 | loss:6.283990154266357\n",
      "epoch 1 | iter: 341/1327 | pp: 698.527055689728 | loss:6.668487182642733\n",
      "epoch 1 | iter: 361/1327 | pp: 694.3865066763101 | loss:6.540165210222559\n",
      "epoch 1 | iter: 381/1327 | pp: 629.6986809761512 | loss:6.120818073095288\n",
      "epoch 1 | iter: 401/1327 | pp: 623.1445582704979 | loss:6.52465873704691\n",
      "epoch 1 | iter: 421/1327 | pp: 611.3680180363739 | loss:6.483787406214646\n",
      "epoch 1 | iter: 441/1327 | pp: 548.1187982387319 | loss:6.095679080951293\n",
      "epoch 1 | iter: 461/1327 | pp: 551.7958776705947 | loss:6.453576234699493\n",
      "epoch 1 | iter: 481/1327 | pp: 516.2542785032033 | loss:6.339563766307963\n",
      "epoch 1 | iter: 501/1327 | pp: 527.1480688438759 | loss:6.324444647750684\n",
      "epoch 1 | iter: 521/1327 | pp: 512.8486829341562 | loss:6.056890612437523\n",
      "epoch 1 | iter: 541/1327 | pp: 532.0220809111084 | loss:6.4225941851202935\n",
      "epoch 1 | iter: 561/1327 | pp: 485.4407320897415 | loss:6.137812493559239\n",
      "epoch 1 | iter: 581/1327 | pp: 439.8290271327147 | loss:6.306331829637887\n",
      "epoch 1 | iter: 601/1327 | pp: 556.8797993339163 | loss:6.368053568582483\n",
      "epoch 1 | iter: 621/1327 | pp: 514.0812937536483 | loss:6.124354726025569\n",
      "epoch 1 | iter: 641/1327 | pp: 463.0648364107036 | loss:5.976927646376592\n",
      "epoch 1 | iter: 661/1327 | pp: 447.65632941771656 | loss:5.9677075189312125\n",
      "epoch 1 | iter: 681/1327 | pp: 385.1131280181632 | loss:6.213688046184938\n",
      "epoch 1 | iter: 701/1327 | pp: 418.62649426680724 | loss:6.08128503936476\n",
      "epoch 1 | iter: 721/1327 | pp: 425.9666794706152 | loss:5.861887860912829\n",
      "epoch 1 | iter: 741/1327 | pp: 362.97981682579115 | loss:5.842334512798115\n",
      "epoch 1 | iter: 761/1327 | pp: 376.9993178785824 | loss:5.908585192255144\n",
      "epoch 1 | iter: 781/1327 | pp: 368.0669345599994 | loss:5.696240929378463\n",
      "epoch 1 | iter: 801/1327 | pp: 394.04459852679855 | loss:5.990340605933951\n",
      "epoch 1 | iter: 821/1327 | pp: 364.65971470605496 | loss:5.997794628185885\n",
      "epoch 1 | iter: 841/1327 | pp: 374.43065486056355 | loss:6.066948834628399\n",
      "epoch 1 | iter: 861/1327 | pp: 369.59913070329185 | loss:5.732272186135607\n",
      "epoch 1 | iter: 881/1327 | pp: 342.57711374426725 | loss:5.691715489721059\n",
      "epoch 1 | iter: 901/1327 | pp: 397.73618237093234 | loss:5.9225725558453375\n",
      "epoch 1 | iter: 921/1327 | pp: 366.0119988374543 | loss:5.9622362871787375\n",
      "epoch 1 | iter: 941/1327 | pp: 362.9499470641717 | loss:5.87440572916264\n",
      "epoch 1 | iter: 961/1327 | pp: 389.1000905577058 | loss:5.915029372523672\n",
      "epoch 1 | iter: 981/1327 | pp: 363.93236766335866 | loss:5.948919799257668\n",
      "epoch 1 | iter: 1001/1327 | pp: 316.0343740186157 | loss:5.780571269792105\n",
      "epoch 1 | iter: 1021/1327 | pp: 355.04789633690064 | loss:5.744244366218253\n",
      "epoch 1 | iter: 1041/1327 | pp: 336.5770211746065 | loss:5.610450587359872\n",
      "epoch 1 | iter: 1061/1327 | pp: 326.84917015218264 | loss:5.653215970568625\n",
      "epoch 1 | iter: 1081/1327 | pp: 289.1346833356949 | loss:5.766070218636388\n",
      "epoch 1 | iter: 1101/1327 | pp: 317.8838832300636 | loss:5.9004853838159965\n",
      "epoch 1 | iter: 1121/1327 | pp: 364.04274800865653 | loss:5.798105781885263\n",
      "epoch 1 | iter: 1141/1327 | pp: 337.7102419223324 | loss:6.012674589836305\n",
      "epoch 1 | iter: 1161/1327 | pp: 318.1007566528083 | loss:5.605384378966243\n",
      "epoch 1 | iter: 1181/1327 | pp: 306.8114172797058 | loss:5.545160650997422\n",
      "epoch 1 | iter: 1201/1327 | pp: 261.87337605476176 | loss:5.588625434519656\n",
      "epoch 1 | iter: 1221/1327 | pp: 264.842563267802 | loss:5.842810751246954\n",
      "epoch 1 | iter: 1241/1327 | pp: 304.43677208122506 | loss:5.89401236610221\n",
      "epoch 1 | iter: 1261/1327 | pp: 280.5638946700118 | loss:5.516977598165561\n",
      "epoch 1 | iter: 1281/1327 | pp: 290.9070344243062 | loss:5.921560403687347\n",
      "epoch 1 | iter: 1301/1327 | pp: 360.69603639095845 | loss:5.826652901723449\n",
      "epoch 1 | iter: 1321/1327 | pp: 332.8440968777086 | loss:5.842520783769765\n",
      "epoch 2 | iter: 1/1327 | pp: 341.01978396330753 | loss:5.855428468821836\n",
      "epoch 2 | iter: 21/1327 | pp: 321.84582953506464 | loss:5.869622282832861\n",
      "epoch 2 | iter: 41/1327 | pp: 316.2984080039643 | loss:5.694617509901789\n",
      "epoch 2 | iter: 61/1327 | pp: 285.65512343349667 | loss:5.594931585537935\n",
      "epoch 2 | iter: 81/1327 | pp: 269.5012904250744 | loss:5.32641323808115\n",
      "epoch 2 | iter: 101/1327 | pp: 254.74476962229562 | loss:5.574574873004375\n",
      "epoch 2 | iter: 121/1327 | pp: 269.9243315286221 | loss:5.4611064884721\n",
      "epoch 2 | iter: 141/1327 | pp: 293.25190370142565 | loss:5.789831714277555\n",
      "epoch 2 | iter: 161/1327 | pp: 313.62927920120444 | loss:5.889344776264791\n",
      "epoch 2 | iter: 181/1327 | pp: 324.8186279355067 | loss:5.599475038970288\n",
      "epoch 2 | iter: 201/1327 | pp: 300.37194911451934 | loss:5.6807752618792335\n",
      "epoch 2 | iter: 221/1327 | pp: 301.08740896308126 | loss:5.661936706413648\n",
      "epoch 2 | iter: 241/1327 | pp: 287.65489913922875 | loss:5.434316220908658\n",
      "epoch 2 | iter: 261/1327 | pp: 306.10194758548255 | loss:5.843855817011956\n",
      "epoch 2 | iter: 281/1327 | pp: 305.002314976314 | loss:5.555723494360822\n",
      "epoch 2 | iter: 301/1327 | pp: 273.1649824292132 | loss:5.640093483607551\n",
      "epoch 2 | iter: 321/1327 | pp: 236.82006016233558 | loss:5.093484611374858\n",
      "epoch 2 | iter: 341/1327 | pp: 277.2235495374214 | loss:5.740855736020021\n",
      "epoch 2 | iter: 361/1327 | pp: 314.0891778513431 | loss:5.292346077176543\n",
      "epoch 2 | iter: 381/1327 | pp: 259.77990838609236 | loss:5.437984730432009\n",
      "epoch 2 | iter: 401/1327 | pp: 276.9573516206491 | loss:5.378425371179058\n",
      "epoch 2 | iter: 421/1327 | pp: 263.9151334526346 | loss:5.508270382563517\n",
      "epoch 2 | iter: 441/1327 | pp: 265.57636486149806 | loss:5.6604577007864805\n",
      "epoch 2 | iter: 461/1327 | pp: 266.0178317909756 | loss:5.60413008242779\n",
      "epoch 2 | iter: 481/1327 | pp: 258.2753053214649 | loss:5.490162678608571\n",
      "epoch 2 | iter: 501/1327 | pp: 266.1899143560642 | loss:5.6737859877712\n",
      "epoch 2 | iter: 521/1327 | pp: 267.3252465499875 | loss:5.626607859176916\n",
      "epoch 2 | iter: 541/1327 | pp: 273.36456653108246 | loss:5.660766825825891\n",
      "epoch 2 | iter: 561/1327 | pp: 254.67792575602127 | loss:5.511226188793579\n",
      "epoch 2 | iter: 581/1327 | pp: 232.78092369832518 | loss:5.783681807133502\n",
      "epoch 2 | iter: 601/1327 | pp: 307.8683625738172 | loss:5.628027742473351\n",
      "epoch 2 | iter: 621/1327 | pp: 287.5449213090053 | loss:5.408307896810085\n",
      "epoch 2 | iter: 641/1327 | pp: 267.55750733510513 | loss:5.492607684993252\n",
      "epoch 2 | iter: 661/1327 | pp: 254.42589523750706 | loss:5.331596028801703\n",
      "epoch 2 | iter: 681/1327 | pp: 214.19958961419275 | loss:5.759025937282879\n",
      "epoch 2 | iter: 701/1327 | pp: 247.199787769854 | loss:5.708349408243417\n",
      "epoch 2 | iter: 721/1327 | pp: 254.78879685220772 | loss:5.621923425128584\n",
      "epoch 2 | iter: 741/1327 | pp: 213.6766615247418 | loss:5.449423560525923\n",
      "epoch 2 | iter: 761/1327 | pp: 224.79800278232702 | loss:4.8870111686041176\n",
      "epoch 2 | iter: 781/1327 | pp: 226.3615763958043 | loss:5.143982109929867\n",
      "epoch 2 | iter: 801/1327 | pp: 244.35812200155542 | loss:5.381603708847792\n",
      "epoch 2 | iter: 821/1327 | pp: 228.91873370432927 | loss:5.39486627028016\n",
      "epoch 2 | iter: 841/1327 | pp: 239.4654985801116 | loss:5.6077115812871074\n",
      "epoch 2 | iter: 861/1327 | pp: 243.56505596098884 | loss:5.496222933158943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 | iter: 881/1327 | pp: 220.8607635109063 | loss:5.221474100608279\n",
      "epoch 2 | iter: 901/1327 | pp: 260.35675118662823 | loss:5.567309299672488\n",
      "epoch 2 | iter: 921/1327 | pp: 240.55274572223217 | loss:5.458218446980214\n",
      "epoch 2 | iter: 941/1327 | pp: 248.53431445890752 | loss:5.323691996658786\n",
      "epoch 2 | iter: 961/1327 | pp: 268.7339025487138 | loss:5.483283473978684\n",
      "epoch 2 | iter: 981/1327 | pp: 250.82279571575233 | loss:5.683613821762597\n",
      "epoch 2 | iter: 1001/1327 | pp: 215.15641885801716 | loss:5.446313451620434\n",
      "epoch 2 | iter: 1021/1327 | pp: 247.19550566834096 | loss:5.35348371082102\n",
      "epoch 2 | iter: 1041/1327 | pp: 237.0117046418032 | loss:5.252388260577593\n",
      "epoch 2 | iter: 1061/1327 | pp: 223.32390494156274 | loss:5.32181112405158\n",
      "epoch 2 | iter: 1081/1327 | pp: 196.05302318622827 | loss:5.30019205498322\n",
      "epoch 2 | iter: 1101/1327 | pp: 221.32310330869927 | loss:5.347302588396727\n",
      "epoch 2 | iter: 1121/1327 | pp: 261.62889936366344 | loss:5.517262595827717\n",
      "epoch 2 | iter: 1141/1327 | pp: 239.4102186174445 | loss:5.633174210683044\n",
      "epoch 2 | iter: 1161/1327 | pp: 233.37322905552443 | loss:5.577577205215462\n",
      "epoch 2 | iter: 1181/1327 | pp: 224.78655217726558 | loss:5.562627941739462\n",
      "epoch 2 | iter: 1201/1327 | pp: 191.18050601352772 | loss:4.940952304777716\n",
      "epoch 2 | iter: 1221/1327 | pp: 189.1289963978285 | loss:5.37913717540929\n",
      "epoch 2 | iter: 1241/1327 | pp: 223.12539671626163 | loss:5.58216007065583\n",
      "epoch 2 | iter: 1261/1327 | pp: 211.00201472133514 | loss:5.344077096115621\n",
      "epoch 2 | iter: 1281/1327 | pp: 211.21394084053634 | loss:5.316953003921413\n",
      "epoch 2 | iter: 1301/1327 | pp: 267.18560618916297 | loss:5.453186287445216\n",
      "epoch 2 | iter: 1321/1327 | pp: 250.58651128238571 | loss:5.4362167998102295\n",
      "epoch 3 | iter: 1/1327 | pp: 258.2150136691222 | loss:5.6967108823435515\n",
      "epoch 3 | iter: 21/1327 | pp: 241.5129280754886 | loss:5.251494231818942\n",
      "epoch 3 | iter: 41/1327 | pp: 238.95527106674734 | loss:5.512453912071693\n",
      "epoch 3 | iter: 61/1327 | pp: 215.1246531833787 | loss:5.2535598851931615\n",
      "epoch 3 | iter: 81/1327 | pp: 207.0632861166688 | loss:5.525230006679153\n",
      "epoch 3 | iter: 101/1327 | pp: 191.40392665128203 | loss:5.4291284360638485\n",
      "epoch 3 | iter: 121/1327 | pp: 206.94303469685823 | loss:5.374399659764355\n",
      "epoch 3 | iter: 141/1327 | pp: 222.9273073434903 | loss:5.478009453723872\n",
      "epoch 3 | iter: 161/1327 | pp: 240.0110664883913 | loss:5.657820683978498\n",
      "epoch 3 | iter: 181/1327 | pp: 251.35033895182775 | loss:5.5507154707431\n",
      "epoch 3 | iter: 201/1327 | pp: 234.80679895687894 | loss:5.583735964638846\n",
      "epoch 3 | iter: 221/1327 | pp: 236.10365540069677 | loss:5.309897174416276\n",
      "epoch 3 | iter: 241/1327 | pp: 226.93106481107029 | loss:5.138037550962534\n",
      "epoch 3 | iter: 261/1327 | pp: 235.53765404801393 | loss:5.53525188280336\n",
      "epoch 3 | iter: 281/1327 | pp: 242.73357841691768 | loss:5.521466470444586\n",
      "epoch 3 | iter: 301/1327 | pp: 216.2093406940098 | loss:5.299024096048628\n",
      "epoch 3 | iter: 321/1327 | pp: 191.71508926542492 | loss:5.06607989651625\n",
      "epoch 3 | iter: 341/1327 | pp: 214.90807530468197 | loss:5.676281568995993\n",
      "epoch 3 | iter: 361/1327 | pp: 261.9342895429946 | loss:5.367111813020726\n",
      "epoch 3 | iter: 381/1327 | pp: 204.90215969777617 | loss:5.335511095521589\n",
      "epoch 3 | iter: 401/1327 | pp: 221.67784247197588 | loss:5.179009116550249\n",
      "epoch 3 | iter: 421/1327 | pp: 209.60780164012147 | loss:5.370840889220646\n",
      "epoch 3 | iter: 441/1327 | pp: 211.7201062198521 | loss:5.472150891138174\n",
      "epoch 3 | iter: 461/1327 | pp: 220.18679979222824 | loss:5.5015526048161\n",
      "epoch 3 | iter: 481/1327 | pp: 214.26053529217577 | loss:5.073808812054932\n",
      "epoch 3 | iter: 501/1327 | pp: 222.1538896844036 | loss:5.331654232721267\n",
      "epoch 3 | iter: 521/1327 | pp: 225.3711228151425 | loss:5.324955119580544\n",
      "epoch 3 | iter: 541/1327 | pp: 229.44295571497062 | loss:5.651767183181447\n",
      "epoch 3 | iter: 561/1327 | pp: 212.40457366231195 | loss:5.403141999020091\n",
      "epoch 3 | iter: 581/1327 | pp: 193.07322696545643 | loss:5.464575171666843\n",
      "epoch 3 | iter: 601/1327 | pp: 260.65508753933403 | loss:5.606885386018881\n",
      "epoch 3 | iter: 621/1327 | pp: 242.57155636309315 | loss:5.4812944551602625\n",
      "epoch 3 | iter: 641/1327 | pp: 222.47428932778402 | loss:5.19074472582899\n",
      "epoch 3 | iter: 661/1327 | pp: 214.15771111846695 | loss:5.276992206708465\n",
      "epoch 3 | iter: 681/1327 | pp: 175.93359175766784 | loss:5.416092179438232\n",
      "epoch 3 | iter: 701/1327 | pp: 206.9287533284756 | loss:5.402450184606216\n",
      "epoch 3 | iter: 721/1327 | pp: 215.3659476195899 | loss:5.3079438421402925\n",
      "epoch 3 | iter: 741/1327 | pp: 180.8330005983414 | loss:5.034668063010488\n",
      "epoch 3 | iter: 761/1327 | pp: 194.449326276372 | loss:4.872487685451093\n",
      "epoch 3 | iter: 781/1327 | pp: 186.07089776620077 | loss:5.155239193614023\n",
      "epoch 3 | iter: 801/1327 | pp: 205.34269519614452 | loss:5.324245481528176\n",
      "epoch 3 | iter: 821/1327 | pp: 195.68737108764066 | loss:5.200902191183455\n",
      "epoch 3 | iter: 841/1327 | pp: 199.70500001060074 | loss:5.353796756887293\n",
      "epoch 3 | iter: 861/1327 | pp: 210.27244181510437 | loss:5.2856587340928884\n",
      "epoch 3 | iter: 881/1327 | pp: 189.0068160905527 | loss:5.470039992045204\n",
      "epoch 3 | iter: 901/1327 | pp: 222.39754777210175 | loss:5.3203638891618175\n",
      "epoch 3 | iter: 921/1327 | pp: 207.9526388211461 | loss:5.206580528908931\n",
      "epoch 3 | iter: 941/1327 | pp: 215.89530863222618 | loss:5.104853695988443\n",
      "epoch 3 | iter: 961/1327 | pp: 228.0997729417236 | loss:5.404392851654619\n",
      "epoch 3 | iter: 981/1327 | pp: 210.38222678749173 | loss:5.687513938825453\n",
      "epoch 3 | iter: 1001/1327 | pp: 184.6285222225171 | loss:5.315268329818427\n",
      "epoch 3 | iter: 1021/1327 | pp: 212.59820744669463 | loss:5.31532969631501\n",
      "epoch 3 | iter: 1041/1327 | pp: 204.62644200022842 | loss:5.19300104761413\n",
      "epoch 3 | iter: 1061/1327 | pp: 187.0066341420619 | loss:5.321147305585244\n",
      "epoch 3 | iter: 1081/1327 | pp: 166.62779901677527 | loss:5.0297100763922415\n",
      "epoch 3 | iter: 1101/1327 | pp: 185.7910299430067 | loss:5.298988685742204\n",
      "epoch 3 | iter: 1121/1327 | pp: 224.36388729431872 | loss:5.575073354192105\n",
      "epoch 3 | iter: 1141/1327 | pp: 204.60786325833334 | loss:5.4901505478531805\n",
      "epoch 3 | iter: 1161/1327 | pp: 199.24772922317769 | loss:5.322464099271081\n",
      "epoch 3 | iter: 1181/1327 | pp: 192.23163306282277 | loss:5.509756951433249\n",
      "epoch 3 | iter: 1201/1327 | pp: 167.4185367825003 | loss:5.12409229225109\n",
      "epoch 3 | iter: 1221/1327 | pp: 159.3733204388736 | loss:5.463703465294342\n",
      "epoch 3 | iter: 1241/1327 | pp: 189.812146971548 | loss:5.656632354570902\n",
      "epoch 3 | iter: 1261/1327 | pp: 184.309246666437 | loss:5.348430797506922\n",
      "epoch 3 | iter: 1281/1327 | pp: 180.78991348015603 | loss:5.37403980846733\n",
      "epoch 3 | iter: 1301/1327 | pp: 230.01856059079896 | loss:5.46925052078025\n",
      "epoch 3 | iter: 1321/1327 | pp: 216.9043863538177 | loss:5.543307360018038\n",
      "epoch 4 | iter: 1/1327 | pp: 218.77606335146348 | loss:5.436255750605571\n",
      "epoch 4 | iter: 21/1327 | pp: 211.39867650023797 | loss:5.47004132855879\n",
      "epoch 4 | iter: 41/1327 | pp: 202.15141137059288 | loss:5.1722123355751055\n",
      "epoch 4 | iter: 61/1327 | pp: 188.4145978747958 | loss:5.134102819771756\n",
      "epoch 4 | iter: 81/1327 | pp: 175.60223422399773 | loss:5.045327813276755\n",
      "epoch 4 | iter: 101/1327 | pp: 165.94455072699768 | loss:5.2240910639602225\n",
      "epoch 4 | iter: 121/1327 | pp: 178.29730868203046 | loss:5.17584010069666\n",
      "epoch 4 | iter: 141/1327 | pp: 191.19413627989582 | loss:5.4539188011226365\n",
      "epoch 4 | iter: 161/1327 | pp: 206.647878626463 | loss:5.136873963171882\n",
      "epoch 4 | iter: 181/1327 | pp: 218.78412896027 | loss:5.393530201705705\n",
      "epoch 4 | iter: 201/1327 | pp: 203.42129489347593 | loss:5.25527495826424\n",
      "epoch 4 | iter: 221/1327 | pp: 207.96592035884896 | loss:5.0001123451126475\n",
      "epoch 4 | iter: 241/1327 | pp: 202.0206060186772 | loss:5.449337119530246\n",
      "epoch 4 | iter: 261/1327 | pp: 203.43504189636735 | loss:5.396641032315341\n",
      "epoch 4 | iter: 281/1327 | pp: 212.83434536655423 | loss:5.233207152523298\n",
      "epoch 4 | iter: 301/1327 | pp: 187.90204819930935 | loss:5.083779671801666\n",
      "epoch 4 | iter: 321/1327 | pp: 165.1713126211538 | loss:5.086346515976916\n",
      "epoch 4 | iter: 341/1327 | pp: 179.5806530759466 | loss:5.402643739857834\n",
      "epoch 4 | iter: 361/1327 | pp: 231.7768812392549 | loss:5.0753297337290135\n",
      "epoch 4 | iter: 381/1327 | pp: 179.582351143612 | loss:4.973277030120246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 | iter: 401/1327 | pp: 193.21032044871646 | loss:5.344774730020269\n",
      "epoch 4 | iter: 421/1327 | pp: 178.06596081771121 | loss:5.14663705683043\n",
      "epoch 4 | iter: 441/1327 | pp: 182.91142652563818 | loss:5.147847845662909\n",
      "epoch 4 | iter: 461/1327 | pp: 191.0874733226447 | loss:5.334453158053969\n",
      "epoch 4 | iter: 481/1327 | pp: 195.23159416955932 | loss:5.058846635646748\n",
      "epoch 4 | iter: 501/1327 | pp: 194.51342990058356 | loss:5.091798987026892\n",
      "epoch 4 | iter: 521/1327 | pp: 198.87414301254793 | loss:4.810411796766566\n",
      "epoch 4 | iter: 541/1327 | pp: 201.7901616440902 | loss:5.378550995116737\n",
      "epoch 4 | iter: 561/1327 | pp: 196.4630051848653 | loss:5.110838300638113\n",
      "epoch 4 | iter: 581/1327 | pp: 173.92679010686572 | loss:5.195570663058814\n",
      "epoch 4 | iter: 601/1327 | pp: 230.43075576350103 | loss:5.492239521505378\n",
      "epoch 4 | iter: 621/1327 | pp: 220.21185210508241 | loss:5.408539423644377\n",
      "epoch 4 | iter: 641/1327 | pp: 202.52194361779613 | loss:5.268616684990702\n",
      "epoch 4 | iter: 661/1327 | pp: 189.32678282895367 | loss:5.34695278990835\n",
      "epoch 4 | iter: 681/1327 | pp: 156.40373566734303 | loss:5.20065018438468\n",
      "epoch 4 | iter: 701/1327 | pp: 185.7686576896128 | loss:5.063961661314435\n",
      "epoch 4 | iter: 721/1327 | pp: 193.38114790450624 | loss:5.188124518479973\n",
      "epoch 4 | iter: 741/1327 | pp: 164.4399685943666 | loss:5.061005378633937\n",
      "epoch 4 | iter: 761/1327 | pp: 171.35429303549964 | loss:4.694305326377341\n",
      "epoch 4 | iter: 781/1327 | pp: 165.23644866194087 | loss:5.1273409348926675\n",
      "epoch 4 | iter: 801/1327 | pp: 183.6717180473287 | loss:5.204536850278174\n",
      "epoch 4 | iter: 821/1327 | pp: 179.2631378608474 | loss:5.207090122598789\n",
      "epoch 4 | iter: 841/1327 | pp: 180.11459362791717 | loss:5.377069775389142\n",
      "epoch 4 | iter: 861/1327 | pp: 190.08764428386868 | loss:5.256029004569739\n",
      "epoch 4 | iter: 881/1327 | pp: 168.45138766472235 | loss:5.220636985141319\n",
      "epoch 4 | iter: 901/1327 | pp: 203.33609533998168 | loss:5.2912004170895255\n",
      "epoch 4 | iter: 921/1327 | pp: 186.6711245248232 | loss:5.13443421685536\n",
      "epoch 4 | iter: 941/1327 | pp: 194.12078931688976 | loss:5.254504113458347\n",
      "epoch 4 | iter: 961/1327 | pp: 201.26216130859345 | loss:5.33592814362109\n",
      "epoch 4 | iter: 981/1327 | pp: 190.386430071366 | loss:5.345606274398244\n",
      "epoch 4 | iter: 1001/1327 | pp: 172.28622738819135 | loss:4.966620519200868\n",
      "epoch 4 | iter: 1021/1327 | pp: 193.3407838121833 | loss:4.97219327670942\n",
      "epoch 4 | iter: 1041/1327 | pp: 185.43771565963965 | loss:5.159476607707995\n",
      "epoch 4 | iter: 1061/1327 | pp: 166.14205428430236 | loss:5.069674928120636\n",
      "epoch 4 | iter: 1081/1327 | pp: 150.9285763041627 | loss:5.089225919073007\n",
      "epoch 4 | iter: 1101/1327 | pp: 163.5261845496647 | loss:5.0654540225593205\n",
      "epoch 4 | iter: 1121/1327 | pp: 197.63624357237583 | loss:5.411531377990614\n",
      "epoch 4 | iter: 1141/1327 | pp: 186.8934701898602 | loss:5.128119589116219\n",
      "epoch 4 | iter: 1161/1327 | pp: 178.67355927239817 | loss:5.1671991839876865\n",
      "epoch 4 | iter: 1181/1327 | pp: 173.47009310094347 | loss:5.135469327803169\n",
      "epoch 4 | iter: 1201/1327 | pp: 155.44106748051433 | loss:4.863827472779272\n",
      "epoch 4 | iter: 1221/1327 | pp: 143.88109712780388 | loss:5.098808754877791\n",
      "epoch 4 | iter: 1241/1327 | pp: 169.85050718293726 | loss:5.075480245376779\n",
      "epoch 4 | iter: 1261/1327 | pp: 170.00431158183773 | loss:5.066662473299158\n",
      "epoch 4 | iter: 1281/1327 | pp: 165.02453338110504 | loss:5.398940321664559\n",
      "epoch 4 | iter: 1301/1327 | pp: 205.06182962352116 | loss:5.4297286530923365\n",
      "epoch 4 | iter: 1321/1327 | pp: 195.87102514910424 | loss:5.169989105588091\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):\n",
    "    for iter in range(max_iters):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i , offset in enumerate(offsets):\n",
    "                batch_x[i,t] = xs[( time_idx + offset) % data_size]\n",
    "                batch_t[i,t] = ts[( time_idx + offset) % data_size]\n",
    "            time_idx +=1\n",
    "        \n",
    "    \n",
    "        batch_x_tensor = torch.tensor(batch_x)\n",
    "        batch_t_tensor = torch.tensor(batch_t)\n",
    "        batch_t_tensor = to_one_hot(batch_t_tensor, vocab_size)\n",
    "        \n",
    "        out , (h_00 , c_00), (h_01 , c_01) = model(batch_x_tensor, h_00, c_00, h_01, c_01)\n",
    "        \n",
    "        \n",
    "        batch_t_tensor = batch_t_tensor.reshape(batch_size*time_size,-1)\n",
    "        \n",
    "        loss = criterion(out, batch_t_tensor) #要保证第二个维度是 vocab_size\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value = max_grad)\n",
    "    \n",
    "        optimizer.step()\n",
    "        h_00 = h_00.detach()\n",
    "        c_00 = c_00.detach()  #BPTT截断反向传播\n",
    "        h_01 = h_01.detach()\n",
    "        c_01 = c_01.detach()\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "    \n",
    "        if iter % 20 ==0:\n",
    "            ppl = torch.exp(total_loss / loss_count)\n",
    "            print('epoch {} | iter: {}/1327 | pp: {} | loss:{}'.format(epoch+1,iter+1,ppl,loss.item()))\n",
    "            ppl_list.append(float(ppl))\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4343c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
